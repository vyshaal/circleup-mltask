{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Question 2 - Use the user, user_features & model_test_file datasets</h1>\n",
    "<h3> <font color='crimson'>1. Write a function that takes as input the user features and outputs the predicted response variable (e.g. content_created) found in the user dataset.</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as plt\n",
    "import plotly.figure_factory as ff\n",
    "plt.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset path\n",
    "excel_file = \"../spreadsheets/Data science take home Datasets.xlsx\"\n",
    "\n",
    "# loading the sheets 'user', 'user_features', 'model_test_file' into dataframes\n",
    "user_df = pd.read_excel(excel_file, \"user\")\n",
    "user_features_df = pd.read_excel(excel_file, \"user_features\")\n",
    "model_test_file_df = pd.read_excel(excel_file, \"model_test_file\")\n",
    "\n",
    "# indexing the dataframes with user_id\n",
    "user_df.set_index('user_id', inplace=True)\n",
    "user_features_df.set_index('user_id', inplace=True)\n",
    "model_test_file_df.set_index('user_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color='crimson'> Mapping function from input user features to the output predicted response variable</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating a dataset comprising features and a response variable (var_1, var_2, ... var_12, response)\n",
    "dataset = user_features_df.copy()\n",
    "dataset['response'] = dataset.index.map(lambda x: user_df.loc[x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of occurrences of each response variable: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.849351\n",
       "1    0.150649\n",
       "Name: response, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ratio of output response variable\n",
    "print(\"Number of occurrences of each response variable: \")\n",
    "dataset['response'].value_counts()/sum(dataset['response'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>From above, it's clearly evident that our response variable is skewed. We propose Synthetic Minority Oversampling Technique (SMOTE), a over-sampling method. SMOTE creates synthetic samples of minority class without just duplicating them. SMOTE does this by selecting similar records and altering that record one column at a time by a random amount within the difference to the neighbouring records.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# partitioning the dataset into training and testing in the ration 70:30\n",
    "training_features, test_features, training_target, test_target, = \\\n",
    "    train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=1/3, stratify=dataset.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We are going to use RandomForestClassifier to model the data after oversampling the minority class. RandomForest builds multiple decision trees and merges them together to get a more accurate and stable prediction. RandomForest also returns feature importance which determines the most important features used in determination of response. RandomForestClassifier handles the problem of overfitting by finding the mode of responses of all the trees. Also building of each tree is independent to one another, so trees can be built in parallel. Feature Scaling is also not required for RandomForest because split takes place on one feature at a time. In \"general\", RandomForest are hard to beat in terms of performance. RandomForest works effeciently for a small dataset too.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('oversample', SMOTE(k=None, k_neighbors=5, kind='regular', m=None, m_neighbors=10, n_jobs=1,\n",
       "   out_step=0.5, random_state=None, ratio='auto', svm_estimator=None)), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='a..._jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'oversample__ratio': [0.25, 0.5, 1], 'clf__max_depth': [3, 5], 'clf__max_features': ['sqrt', 'log2'], 'clf__n_estimators': [25, 50, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data to be transformed is chained together through a pipeline\n",
    "pipe = Pipeline([('oversample', SMOTE()),\n",
    "                 ('clf', RandomForestClassifier(n_jobs=-1))])\n",
    "\n",
    "# stratifiedkfold makes sure that folds are made by preserved the percentage of samples for each class\n",
    "skf = StratifiedKFold()\n",
    "\n",
    "# range of each parameter to be explored for tuning\n",
    "param_grid = {'oversample__ratio': [0.25, 0.5, 1],      # ratio of majority class to minority class\n",
    "              'clf__max_depth': [3, 5],                 # maximum depth of each tree\n",
    "              'clf__max_features': ['sqrt', 'log2'],    # maximum number of features to be considered at each split\n",
    "              'clf__n_estimators': [25, 50, 100]}       # number of trees in the forest\n",
    "\n",
    "# using F1_Score as scoring criterion for scoring and stratifiedkfold for cross validation\n",
    "grid = GridSearchCV(pipe, param_grid, return_train_score=False,\n",
    "                    n_jobs=-1, scoring='f1', cv=skf)\n",
    "\n",
    "grid.fit(training_features, training_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> In general, we use accuracy as scoring criterion for parameter tuning. However, we have an imbalanced dataset. In such cases, accuracy can't be trusted in evaluating a model because of a very high value of True Negatives. We can use Recall as a scoring criterion in cases like Fraud Detection, Cancer Detection so that we get high value of True Positives. In such cases weight of True Positives is very high and so we use only recall without precision as scoring criterion. In our case, we don't have to give high weights to True Positive. So we use both precision and recall as scoring criterion, the trade-off between precision and recall is achieved through F1_Score (harmonic mean of precision and recall).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> <font color='crimson'> 2. Report the predicted response for the users in the model_test_file. If you use any visualizations/metrics to validate the model please include them in the report. Please explain the reasoning behind the technique you used to build the model.</font></h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> <font color='crimson'>Predicted response for the users in the model_test_file</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted response for the users in model_test_file: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "154472    0\n",
       "151147    1\n",
       "10543     1\n",
       "136986    0\n",
       "137008    0\n",
       "137059    0\n",
       "137253    0\n",
       "137323    0\n",
       "171652    0\n",
       "137341    0\n",
       "137768    0\n",
       "137822    0\n",
       "137879    0\n",
       "137984    1\n",
       "138103    0\n",
       "138200    0\n",
       "138405    1\n",
       "138439    0\n",
       "138473    0\n",
       "138483    0\n",
       "138582    0\n",
       "138606    0\n",
       "138738    0\n",
       "138948    0\n",
       "139606    0\n",
       "149425    1\n",
       "149419    0\n",
       "150894    1\n",
       "141733    0\n",
       "144653    0\n",
       "147263    0\n",
       "147793    0\n",
       "148663    1\n",
       "149065    0\n",
       "149643    0\n",
       "149792    0\n",
       "10420     0\n",
       "152535    0\n",
       "153169    0\n",
       "153389    0\n",
       "153395    0\n",
       "154070    0\n",
       "154777    0\n",
       "168361    0\n",
       "168689    0\n",
       "168840    0\n",
       "168845    0\n",
       "168862    1\n",
       "168864    0\n",
       "168989    0\n",
       "168991    0\n",
       "169591    0\n",
       "169636    0\n",
       "169853    0\n",
       "259509    0\n",
       "260608    1\n",
       "602671    0\n",
       "602672    0\n",
       "8107      0\n",
       "Name: prediction, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted response for the users in model_test_file\n",
    "model_test_file_df['prediction'] = grid.predict(model_test_file_df.iloc[:, :12])\n",
    "print(\"Predicted response for the users in model_test_file: \")\n",
    "model_test_file_df['prediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color='crimson'> Visualizations/Metrics to validate the model</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters after tuning are: {'clf__max_depth': 3, 'clf__max_features': 'sqrt', 'clf__n_estimators': 100, 'oversample__ratio': 0.5}\n",
      "F1_Score of the model is: 0.648985959438\n",
      "Accuracy of the model is: 0.873096446701\n",
      "Precision of the model is: 0.55614973262\n",
      "Recall of the model is: 0.779026217228\n",
      "Classification Report on test data: \n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.96      0.89      0.78      0.92      0.83      0.70      1506\n",
      "          1       0.56      0.78      0.89      0.65      0.83      0.69       267\n",
      "\n",
      "avg / total       0.90      0.87      0.80      0.88      0.83      0.70      1773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing the classification results\n",
    "test_features_predictions = grid.predict(test_features)\n",
    "print(\"Best parameters after tuning are: \" + str(grid.best_params_))\n",
    "print(\"F1_Score of the model is: \" + str(f1_score(test_target, test_features_predictions)))\n",
    "print(\"Accuracy of the model is: \" + str(accuracy_score(test_target, test_features_predictions)))\n",
    "print(\"Precision of the model is: \" + str(precision_score(test_target, test_features_predictions)))\n",
    "print(\"Recall of the model is: \" + str(recall_score(test_target, test_features_predictions)))\n",
    "print(\"Classification Report on test data: \\n\" + \n",
    "      classification_report_imbalanced(test_target, test_features_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
